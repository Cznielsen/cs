{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Exercises week 2\n",
    "<span style=\"color:red\">\n",
    "Like last week, it is very imporant that you <b>try</b> to solve every exercise. It is not important that you answer correctly. Spend no more than 5-10 min on each exercise. If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.\n",
    "\n",
    "The TA's will be very happy to answer questions during the TA session or on the board.\n",
    "\n",
    "You might want to spend a bit more than 10 min on 5, 6 and 7, they are substantially harder than the rest of the exercises. Do not despair if you cannot solve them, but try to understand the question and pinpoint which parts you do not understand. </span>.\n",
    "\n",
    "# 1. A story about Ben \n",
    "Ben is a student at Aarhus University. Like all other university students Ben has to learn a lot of different stuff. Ben is taking a class called '5 ECTS ON BIRDS' where he has to learn two things\n",
    "\n",
    "   1. During the course, the lecturer gave Ben a photo album with images of different birds and their name. Ben needs to learn the names of all the birds. At the exam, Ben will be given several images for which he needs to name the birds.  \n",
    "    \n",
    "   2. During the course, the lecturer gave Ben several recordings of birds singing. For this task the lecturer doesn't care about names. At the exam, Ben will be given recordings he needs to seperate into two groups based on how similar they sound.    \n",
    "    \n",
    "Ben notices one significant difference between the two tasks. In the first, his supervisor gave him the correct answer for each image. This is called *supervised learning* because the supervisor provides a correct answer. In the second task, his supervisor did not give him a correct answer. Instead, Ben had to group the audio samples based on similarity. This is called *unsupervised learning*, because there is not supervision in terms of correct answers. \n",
    "\n",
    "# 2. Learning Types \n",
    "In this exercise you must distinguish between Supervised Learning and Unsupervised Learning. Imagine you work at a company that sells stuff. The company stores a lot of information on its costumers. For each costumer they save the following 5 attributes:\n",
    "\n",
    "    AGE, SEX, INCOME, RESIDENCE, MONEY USED AT COMPANY\n",
    "\n",
    "<b>Question 1: </b><br>In each of the following examples you should determine if the problem is a Supervised or Unsupervised learning problem.\n",
    "\n",
    "-   The company wants to learn how to predict 'MONEY USED AT COMPANY' given 'AGE', 'SEX', 'INCOME' and 'RESIDENCE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to learn ways of grouping costumers depending on 'AGE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to learn how to predict 'SEX' given 'MONEY SPENT AT COMPANY' and 'AGE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to target different groups of costumers depending on 'AGE', 'INCOME' and 'MONEY SPENT AT COMPANY'. Supervised or Unsupervised?   \n",
    "\n",
    "<br><br>\n",
    "<b>Question 2</b>:<br>\n",
    "In supervised learning the data is of the form $D_{supervised}=\\{(x_1,y_1),...,(x_n,y_n)\\}$. <br>\n",
    "In unsupervised learning we have data of the form $D_{unsupervised}=\\{x_1,...,x_n\\}$.\n",
    "\n",
    "Write the form the data would take in each case from Question 1.\n",
    "\n",
    "HINT: Possible solutions to two of the cases\n",
    "\n",
    "$$D=\\{20\\text{ years},\\;21\\text{ years}, \\;23\\text{ years}, ... \\}$$ \n",
    "$$D=\\{([100\\text{ kr},\\;22\\text{ years}],\\text{ male}),\\;([120\\text{ kr},\\;30\\text{ years}],\\text{ female}), ...\\}$$\n",
    " \n",
    "\n",
    "# 3. The story of Ben continues\n",
    "After passing the exam in '5 ECTS ON BIRDS' Ben goes on to take the prestigious '5 ECTS ON ADVANCED BIRDS'. In this class, Ben has to do the following two things\n",
    "\n",
    "   1. He is given a photo album with images of birds and the family they belong to. At the exam, Ben will get images of birds and asked to tell which bird family they belong to. \n",
    "    \n",
    "   2. He is given a photo album with images of birds and how much they cost. At the exam, Ben will get images of birds and be asked to tell how much the bird would cost to buy. \n",
    "    \n",
    "First of, Ben notices that the two tasks are supervised learning; he gets examples with correct answers. Ben also notices a significant difference between the two tasks. In the first, he has to assign each bird to a class. This is called *classification*. In the second task, he has to assign a real number to each bird. This is called *regression*. \n",
    "\n",
    "\n",
    "# 4. Regression or Classification \n",
    "In Supervised Machine Learning there are regression and classification problems. \n",
    "\n",
    "<b>Question 1</b>: <br>In each of the following examples you should distinguish between regression and classification.\n",
    "\n",
    "-   In the previous question the company wanted to predict 'MONEY SPENT AT COMPANY' from ('AGE', 'SEX', 'INCOME', 'RESIDENCE'). Is that regression or classification?\n",
    "\n",
    "-   Recognizing the color of wine as white, rose or red. Is that regression or classification?\n",
    "\n",
    "-   Predicting a students grade in machine learning as a function of previous grades (on the 12 scale). Is that regression or classification? \n",
    "    \n",
    "-   Predicting industry codes from purpose statements. Regression or classification?\n",
    "<br><br>\n",
    "    \n",
    "<b>Question 2: </b> <br>\n",
    "In supervised learning we want to approximate an unkown target function $f:X\\rightarrow Y$. In regression we could have $Y=\\mathbb{R}$ and in classification we could have $Y=\\{c_1,...,c_k\\}$.\n",
    "\n",
    "What is $Y$ in the above four cases? \n",
    "\n",
    "HINT: Consider $Y=\\{white, rose, red\\}$, $Y=\\{-3, 0, 2, 4, 7, 10, 12\\}$, $Y=\\mathbb{R}$. \n",
    "\n",
    "# 5. Helping the Police catch bad guys (learning vs design) \n",
    "The police would like your help with criminal facial detection. They believe it is possible to classify whether a person is guilty by looking at the persons face. They want you to build a system that does that. To do this they give you a lot of data. The data is of the form\n",
    "\n",
    "    (MUGSHOT, GUILTY)\n",
    "\n",
    "Here '<a href=\"https://en.wikipedia.org/wiki/Mug_shot\">MUGSHOT</a>' is an image and 'GUILTY' is a boolean indicating if the person was guilty or not.  Unfortunately, all the mug shots were taken at a <a href=\"http://www.zastavki.com/pictures/originals/2014/Winter_Snow_in_Paris_Eiffel_Tower_tilted_056634_.jpg\">45 degree angle</a>. \n",
    "\n",
    "Your job is to build a classifier system that determines guilt from facial images. The system you construct has 3 different steps.\n",
    "\n",
    "<div style=\"border: 1px solid #333; padding: 16px; margin: 16px;\">\n",
    "<b>Step 1:</b>\n",
    "You write an algorithm that rotates the face -45 degrees so all suspects faces outwards.<br/><br/>\n",
    "\n",
    "<b>Step 2:</b>\n",
    "After looking at the rotated images you discover that the position of the eyes is always 3cm down in the picture. You decide to build a <a href=\"https://en.wikipedia.org/wiki/Feature_extraction\">feature extractor</a> that looks 3cm down in the image and locates the\n",
    "eyes. From this the feature selector computes the length between the eyes, the vertical distance to the nose, the mouth and the horizontal distance to the ears.<br/><br/>\n",
    "\n",
    "<b>Step 3:</b>\n",
    "You enter the features extracted from images of guilty and innocent people into the perceptron algorithm and construct a classifier (represented by a set of weights).\n",
    "</div>\n",
    "\n",
    "The final system is then the 3-stage algorithm: Rotate, Feature Extract, Classify with weight from step 3.\n",
    "\n",
    "<b>Question 1: </b>Which of the steps is learning and which step is design?\n",
    "\n",
    "    Is step 1 learning or design?\n",
    "    Is step 2 learning or design?\n",
    "    Is step 3 learning or design?\n",
    "\n",
    "<b>Question 2:</b> Do you think our approach will work? Why/Why not?\n",
    "\n",
    "<b>Question 3: </b> Can you extend/alter the problem/algorithm to estimate the length of the prison time given?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Ben gets a great idea (generalization)\n",
    "Ben is still happily following the '5 ECTS ON ADVANCED BIRDS' course. The exam is written, and the professor  gave Ben several old exams sets he can practice on. One day Ben got a brilliant idea. Why couldn't the final exam be one of the tests he already practiced on? Very happy with himself, Ben asked his professor\n",
    "\n",
    "Ben: \"Professor. I got a brilliant idea. See this exam from last year? I solved it perfectly!\" \n",
    "\n",
    "Professor: \"Very good Ben. I am very impressed. \"\n",
    "\n",
    "Ben: \"Why don't you just re-use the exam from last year? This way I'll get a 12 and your boss will think you're the greatest teacher ever!\"\n",
    "\n",
    "Professor: \"I see your point Ben. Unfortunately there is one problem. If I did this, I would not be testing what you learned, but what you remembered!\"\n",
    "\n",
    "Ben: \"I am not sure I understand, professor. \"\n",
    "\n",
    "Professor: \"When I gave you the previous exams, you also got the solutions, right?\"\n",
    "\n",
    "Ben: \"Yes, this was very helpfull. \"\n",
    "\n",
    "Professor: \"If I give you the same exam, you could get a perfect score by just remembering the solutions. But I don't want to test what you remembered, I want to test if you learned something. \"\n",
    "\n",
    "Ben: \"I see. Continue. \"\n",
    "\n",
    "Professor: \"You can't solve a new exam by remembering. You never saw it before! My hope is that by doing the previous years exams you learned something and the final exam tests how well you learned. \"\n",
    "\n",
    "Ben: \"Interesting. So my performance on the previous exams and the final exam set are different things?\"\n",
    "\n",
    "Professor: \"Exactly! If you were taking a course in machine learning, we would call your performance on the previous exams *in-sample error* and denote it by $E_{in}$. \"\n",
    "\n",
    "Ben: \"So I can get perfect *in-sample error*, basically zero error, by just remembering all the correct solutions?\"\n",
    "\n",
    "Professor: \"Exactly! You would get $E_{in}=0$. But what I really care about is how well you perform on exam tests you never saw before. We call this *out-of-sample error* and denote it by $E_{out}$. \"\n",
    "\n",
    "Ben: \"I see. But I feel like my performance on previous exams give me a good idea on how I perform later. It seems like $E_{in}$ and $E_{out}$ are close to each other. \"\n",
    "\n",
    "Professor: \"This is often the case. We say that learning generalizes if $E_{in}$ and $E_{out}$ are close to each other, i.e. $|E_{in}-E_{out}|$ is small. \"\n",
    "\n",
    "Ben: \"But then how do we know this is the case. \"\n",
    "\n",
    "Professor: \"If you followed the machine learning course or read 'Learning from Data', you would know that the Hoeffding bound tells us that $\\Pr[|E_{in}-E_{out}|<\\epsilon]\\le 2M e^{-2\\epsilon^2 N}$. \"\n",
    "\n",
    "Ben: \"I see. This was very helpfull professor, but I'm not exactly sure what $M$ and $N$ are. \"\n",
    "\n",
    "Professor: \"I'd recommend you read pages 18-24 of 'Learning From Data' then. \"\n",
    "\n",
    "\n",
    "\n",
    "# 4. Generalization\n",
    "\n",
    "<b>Question 1: </b> \n",
    "Does the Hoeffding bound give any meaningfull bounds on $E_{in}$ and $E_{out}$ for the perceptron learning model?  \n",
    "\n",
    "HINT: Remember that the hypotesis set is $H=\\{ h(x)=\\text{sign}(w^\\intercal x)\\mid w\\in \\mathbb{R}^d\\}$ and $M=|H|$.\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "    \n",
    "<b>Question 2: </b>\n",
    "We want to learn $f:X \\rightarrow \\{0,\\;1\\}$. Assume $f$ is probabilistic and independent of the input domain $X$ such that\n",
    "\n",
    "$$P(f(x)=1\\mid x) = P(f(x) =1) = 0.2$$\n",
    "\n",
    "I.e. the probability that $f(x)=1$ is independent of $x$. You can think of this as follows: when $f$ is evaluated it ignores the input and flips its private biased coin independently and returns the result. \n",
    "\n",
    "<b>a)</b> What is the best out-of-sample error we can achieve? \n",
    "\n",
    "HINT: Consider $h(x)=1$ and $h(x)=0$. Would a randomized function be better? E.g. $h(x)=\\begin{cases}\n",
    "1\\quad\\text{with probability } 0.2\\\\\n",
    "0\\quad\\text{with probability } 0.8\n",
    "\\end{cases}$\n",
    "\n",
    "<b>b)</b> What if $P(f(x)=1) =0.5$? \n",
    "\n",
    "<b>c)</b> What are the optimal classifiers for these two cases?\n",
    "\n",
    "<b>d)</b> Read and understand the following code. Try experiment with different functions. Do the experiments match your answers to previous questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample estimate of out of sample error ...\n",
      "Wait for it some more....\n",
      "It is: 0.507, Is that close? Can you make a better h?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The unknown target function. \n",
    "def f(x):\n",
    "    r = np.random.rand(1)\n",
    "    return r < 0.5\n",
    "\n",
    "# Our hypothesis (our guess)\n",
    "def h(x): \n",
    "    return 1 < 0.5\n",
    "\n",
    "# Lets try to estimate the error of h. \n",
    "data = np.random.rand(1000)\n",
    "f_data = np.array([f(x) for x in data])\n",
    "h_data = np.array([h(x) for x in data])\n",
    "print('Sample estimate of out of sample error ...')\n",
    "eout_sample = (f_data==h_data).mean()\n",
    "print('Wait for it some more....\\nIt is: {0}, Is that close? Can you make a better h?'.format(eout_sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Problem 1.12 from 'Learning By Data'\n",
    "======================\n",
    "The version presented here is slightly changed. \n",
    "\n",
    "## By hand\n",
    "Given $y_1,\\ldots,y_n\\in \\mathbb{R}$ find $h\\in \\mathbb{R}$ that on average is closest to $y_1,..,y_n$ measured by squared distance (least squares). That is, \n",
    "$$\n",
    "h_\\textrm{mean} =\\textrm{arg}\\min_h \\sum_{i=1}^n (h-y_i)^2\n",
    "$$ \n",
    "<b>Question 1: </b>Show that\n",
    "$h_\\textrm{mean} = \\frac{1}{n} \\sum_{i=1}^n y_i$ is the minimizer. \n",
    "\n",
    "HINT: Computing the <a href=\"https://en.wikipedia.org/wiki/Chain_rule\">derivative</a> may be worth the time and strain on your brain. <br>\n",
    "<b> SVAR: </b>\n",
    "$$\n",
    "f(h) = \\sum{i=1}^n (h-y_i)^2\n",
    "$$\n",
    "\n",
    "\\frac{d f}{d h} =\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<b>Question 2: </b>Consider absolute deviation instead of squared distance, i.e.\n",
    "\n",
    "$$h_\\mathrm{med} =\\textrm{arg}\\min_h \\sum_{i=1}^n |h-y_i|$$ then the median $h_\\mathrm{med} = \\mathrm{median}(y_1,\\dots,y_n)$ is the minimizer. \n",
    "\n",
    "HINT: Computing derivative may be usefull but again, remember $|a|=\\sqrt{a^2}$.  \n",
    "\n",
    "HINT: You can also argue purely algorithmically by thinking what happens with the cost as we sweep *h* from $-\\infty$ to $\\infty$).<br><br>\n",
    "\n",
    "<b>Question 3: </b>What happens to the solutions $h_\\mathrm{mean}, h_\\mathrm{med}$ if we\n",
    "add noise the last element $y_n$, i.e. $y_n = y_n + \\varepsilon$ for\n",
    "$\\varepsilon \\rightarrow \\infty$.\n",
    "\n",
    "Which is more stable for outliers?\n",
    "\n",
    "## In code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD8CAYAAABekO4JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3QjpcUIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bb54cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared cost:\n",
      "squared cost of ha_opt: \t5930.53\n",
      "squared cost of hs_opt: \t5714.91\n",
      "seems hs_opt is best for squared dist so far!\n",
      "\n",
      "Absolute cost:\n",
      "abs cost of hs_opt: \t\t273.09\n",
      "abs cost of ha_opt: \t\t269.56\n",
      "seems ha_opt is best for abs dist so far!\n",
      "\n",
      "You can try to add noise to the largest element if you want - see perhaps np.argmax?\n"
     ]
    }
   ],
   "source": [
    "# Enable plotting directly in the notebook\n",
    "%matplotlib inline\n",
    "# import NumPy and MatplotLib pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample 100 elements from normal distribution\n",
    "mean = 0\n",
    "std_dev = 15\n",
    "samples = 20\n",
    "y = np.random.normal(mean, std_dev, samples) # alex: changed from laplacian to normal. \n",
    "hs_opt = np.mean(y)\n",
    "ha_opt = np.median(y)\n",
    "\n",
    "# Plot the random points and the Mean/Median\n",
    "# Try run the code several times (CTRL + ENTER) and inspect the plot\n",
    "plt.plot(y, np.zeros_like(y), 'x')\n",
    "mean, = plt.plot([hs_opt], [0], 'ro', ms=10, label='Mean')\n",
    "median, = plt.plot([ha_opt], [0], 'g^', ms=10, label='Median')\n",
    "plt.legend(handles=[mean, median])\n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "# Compute and print the different costs. \n",
    "print('Squared cost:')\n",
    "square_hs = np.sum((y-hs_opt)**2)\n",
    "square_ha = np.sum((y-ha_opt)**2)\n",
    "\n",
    "print('squared cost of ha_opt: \\t{0}'.format(round(square_ha,2)))\n",
    "print('squared cost of hs_opt: \\t{0}'.format(round(square_hs,2)))\n",
    "print('seems hs_opt is best for squared dist so far!')\n",
    "\n",
    "print('\\nAbsolute cost:')\n",
    "abs_hs =  np.sum(np.abs((y-hs_opt)))\n",
    "abs_ha =  np.sum(np.abs((y-ha_opt)))\n",
    "print('abs cost of hs_opt: \\t\\t{0}'.format(round(abs_hs,2)))\n",
    "print('abs cost of ha_opt: \\t\\t{0}'.format(round(abs_ha,2)))\n",
    "                 \n",
    "print('seems ha_opt is best for abs dist so far!')\n",
    "\n",
    "print('\\nYou can try to add noise to the largest element if you want - see perhaps np.argmax?')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 6. Linear Regression and Correlation Coefficient\n",
    "Consider data $D=\\{(x_1,y_1),\\dots, (x_n,y_n)\\}$ where $x_i\\in\\mathbb{R}^d$ and $y_i\\in \\mathbb{R}$. Remember that linear regression is given\n",
    "\n",
    "$$w_\\textrm{lin} = \\text{arg min}_w \\sum_{i=1}^n (w^\\intercal x_i + b - y_i)^2$$\n",
    "\n",
    "Linear Regression is described on page 84 in the book, it even has a nice image (Figure 3.3). You might wonder why the above formula has the bias $b$ term and the formula in the book doesn't. This is because of the notational trick described on page 5-7. In this exercise we consider the 1-dimensional case with $b=0$\n",
    "\n",
    "<b>Question 1: </b>Simplify $w_\\text{lin}=\\text{arg min}_w \\sum_{i=1}^n (w^\\intercal x_i + b - y_i)^2$ for the 1-dimensional with $b=0$.<br><br>\n",
    "HINT: The answer is given below, see $w_{1D-in}$. <br><br>\n",
    "\n",
    "<div style=\"border: 1px solid #333; padding:16px; margin: 16px;\"><b>Definition: </b>\n",
    "Given $x,y\\in \\mathbb{R}^n$ the (sample) Correlation Coefficient is defined as<br><br>\n",
    "\n",
    "$$\n",
    "CC(x,y)=\\frac{\\sum_{i=1}^n (x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^n (x_i - \\mu_x)^2 \\sum_{i=1}^n (y_i - \\mu_y)^2}}\\quad\\quad \\text{where} \\quad \\quad \\mu_x = \\frac{1}{n} \\sum_{i=1}^n x_i\\quad\\text{and}\\quad \\mu_y = \\frac{1}{n} \\sum_{i=1}^n y_i \n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "Notice that $\\mu_x$ and $\\mu_y$ are the mean of $x$ and $y$ respectively. The (sample) Correlation Coefficient measures to what extent the two samples vary together (relative to the mean). If two vectors vary the same way it seems plausible that we can use one to estimate the other. \n",
    "\n",
    "But we already know how to predict one value from another using Linear Regression, so it seems that the (sample) Correlation Coeficient and Linear Regression somehow do something similar!\n",
    "\n",
    "<b>Question 2: </b> Prepare an explanation to your fellow students of why Linear Regression and the (sample) Correlation Coefficient seem to do something similar. \n",
    "\n",
    "\n",
    "The 1-dimensional Linear Regression (predict $y$ from number $x$) for bias $b=0$ considered in Question 1 boils down to \n",
    "$$\n",
    "w_\\textrm{1D-lin} = \\text{arg min}_w \\sum_{i=1}^n (w x_i - y_i)^2\n",
    "$$\n",
    "\n",
    "<b>Question 3: </b>\n",
    "Let $x, y\\in \\mathbb{R}^n$ and let $x', y'$ be the normalized versions (zero mean, std. deviation 1), that is \n",
    "\n",
    "$$x'_i = \\frac{(x_i - \\mu_x)}{\\sigma_x}\\quad\\quad \\text{where}\\quad\\quad  \\mu_x = \\frac{1}{n} \\sum_{i=1}^n x_i \\quad\\quad\\text{and}\\quad\\quad \\sigma_x = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu_x)^2}$$\n",
    "\n",
    "The same for $y'$. \n",
    "\n",
    "** Show that the Linear Regression weight wector $w_\\textrm{1D-lin}$ computed on data x' and targets y' is the sample Correlation Coefficient of x' and y' **\n",
    "\n",
    "$$ \\text{arg min}_w \\sum_{i=1}^n(w x_i' - y_i')^2 = CC(x', y')$$\n",
    "\n",
    "HINT: Show that given input vectors $x, y$ (use $x$ to predict $y$) then\n",
    "$$\n",
    "w_\\textrm{1D-lin} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2} = \\frac{\\langle x, y\\rangle}{\\langle x, x\\rangle} = \\frac{x^\\intercal y}{|x|^2}\n",
    "$$\n",
    "by taking the derivative, setting it equal to $0$ and solving for $w$ (as in the Lecture - only that now it is a function of a single variable/vector of length one).\n",
    "Then plugin the definition for $x'$ and $y'$ and see if you can make it fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,1,2,3,1,-1,-2,3,4,-3])\n",
    "y = 3*x -0.5\n",
    "plt.plot(x, y, 'r-x', linewidth=2)\n",
    "def compare(x, y):\n",
    "    w1dlin = np.dot(x,y)/np.dot(x,x)\n",
    "    mux = np.mean(x)\n",
    "    stdx = np.sqrt(np.mean((x-mux)**2))\n",
    "    muy = np.mean(y)\n",
    "    stdy = np.sqrt(np.mean((y-muy)**2))\n",
    "    xp = (x-mux)/stdx\n",
    "    yp = (y-muy)/stdy\n",
    "\n",
    "    correlation_coefficient = np.dot((x-mux),(y-muy))/np.sqrt(np.dot(x-mux,x-mux)*np.dot(y-muy, y-muy))\n",
    "    print('Correlation coeffient of x and y:', correlation_coefficient)\n",
    "    w1d_lin = np.dot(xp, yp)/np.dot(xp, xp)\n",
    "    print(\"w1d_lin of x', y': \", w1d_lin)\n",
    "    \n",
    "compare(x, y)\n",
    "y = np.array([-1,-2,-3,-1,-2,-3,-1,-1,-2,-3,-4,-3])\n",
    "compare(x, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Derivatives\n",
    "\n",
    "\n",
    "In Linear Regression we defined the in sample error as (ignoring the normalizing factor 1/n)\n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 $$ \n",
    "\n",
    "Let $X$ be the the data matrix of shape $n \\times d$ with data point $x_i$ as the $i$'th row. Let $y$ be the label vector of shape $n \\times 1$ with label $y_i$ as the $i$'th entry. Let $w$ be the weight vector of shape $d \\times 1$.  \n",
    "\n",
    "$$X=\\begin{pmatrix}\n",
    "- & x_1^T & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & x_n^T & - \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times d}\\quad\\quad\\quad\n",
    "y=\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times 1}$$\n",
    "\n",
    "The in-sample error rate $E_{in}$ is then equal to \n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 =(Xw-y)^\\intercal (Xw-y)$$\n",
    "\n",
    "\n",
    "In the lecture we proved that for Linear Regression the optimal weight vector $w_\\textrm{lin}$ for minimizing $E_\\textrm{in}$ was $w_\\textrm{lin} = X^\\dagger y$ where $X^\\dagger=(X^\\intercal X)^{-1} X^\\intercal$ is the pseudoinverse. \n",
    "\n",
    "To do this we used facts about the Jacobian. If we have a function $f(z): \\mathbb{R}^{a} \\rightarrow \\mathbb{R}^b$ such that $f(z) = [f_1(z),\\dots, f_b(z)]$ then the Jacobian is the matrix $J_{i,j} = \\frac{\\partial f_i}{\\partial z_j}$ of size $b\\times a$. For example: Let $f(x): R^2 \\rightarrow R^3$ be the function $f([x_1,x_2]) = [x_1, x_2, x_1*x_2]$ then \n",
    "the Jacobian has shape $3 \\times 2$ and looks like\n",
    "\n",
    "$$\n",
    "J_f(x) =\n",
    " \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1  \\\\\n",
    "  x_2  & x_1 \\\\\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In our proof we used the following identities about the Jacobian\n",
    "\n",
    "* $f: R^d \\rightarrow R^d, f(z) = Xz$, the Jacobian $J_f(z)$ is $X$\n",
    "* $g: R^d \\rightarrow R^d, g(z) = z-y$, the Jacobian $J_g(z)$ is $I$\n",
    "* $h: R^d \\rightarrow R, h(z) = z^\\intercal z$, the Jacobian $J_h(z)$ is $2z^\\intercal$\n",
    "\n",
    "Notice that $E_\\textrm{in} = h(g(f(w)))$. With these identities, we compute the gradient $\\nabla E_\\text{in}$ by multiplying the Jacobians of $h$, $g$, and $f$ evaluated at their inputs (abiding the mighty Chain Ruler), and take the transpose. https://en.wikipedia.org/wiki/Chain_rule\n",
    "\n",
    "Since $f(w) = Xw$, $g(f(w)) = g(Xw) = Xw-y $, and $h(g(f(w)) = (Xw-y)^\\intercal (Xw - y)$\n",
    "Then \n",
    "$$\n",
    "\\nabla E_\\textrm{in} = (2(Xw-y)^\\intercal I X)^\\intercal = 2X^\\intercal(Xw- y)\n",
    "$$\n",
    "Solving for the zero vector gives $w_\\textrm{lin}$ (and the 2 factor becomes irrelevant, like any initial 1/n scaling).\n",
    "\n",
    "Your job is to prove the three identities. To simplify matters we have split the proof up in five parts: \n",
    "\n",
    "In the following $x, w, z$ are columns vectors of shape $d \\times 1$, i.e. $x = [x_1,\\dots,x_d]^\\intercal$ and $X$ is an $d \\times d$ matrix\n",
    "* Let $f:\\mathbb{R}^{d_1} \\rightarrow \\mathbb{R}^{d_2}$, what is the shape of the Jacobian $J_f$\n",
    "* Let $f(x) = w^\\intercal x$, what is $\\frac{\\partial f}{\\partial x_1}$?, what is $\\frac{\\partial f}{\\partial x_j}$?. See a pattern. What is the Jacobian J_f?\n",
    "* Let $f(x) = x - z$, Show that the Jacobian of $f$ is the $d \\times d$ identity matrix.\n",
    "* Let $f(w) = Xw$  (function from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^d$). Show that the Jacobian $J_f = X$. Hint: You can think of $f = [f_1,\\dots,f_d]$ where $f_i(w) = x_{i}^\\intercal w$ where $x_i$ is the i'th row of $X$ and start with $\\frac{\\partial f_1}{\\partial w_1}$ to see if a pattern emerges\n",
    "* Let $f(x) = x^\\intercal x$ (the squared norm of $x$). Show that the Jacobian $J_f = 2x^\\intercal$\n",
    "\n",
    "\n",
    "\n",
    "We have implemented that in python below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets check the shapes:  (2, 1) (3, 2) (3, 1)\n",
      "Lets do the forward pass and compute Ein piece by piece\n",
      "In sample error of w is z3 and is: [[41]]\n",
      "The data X, y is fixed so z3 is a function of w only so to minimize we compute derivatives using the chain rule and return\n",
      "The input is a function from R^d -> R - so the Jacobian should have shape 1 x d and the gradient d x 1\n",
      "Lets use reverse the computation and compute derivatices by backpropagating and apply the chain rule\n",
      "See if the shapes fit: (3, 2) (3, 3) (1, 3)\n",
      "Shape of the Jacobiant: (1, 2)\n",
      "Shape of the Gradient: (2, 1)\n",
      "What is the Gradient: \n",
      " [[ 22.]\n",
      " [ 68.]]\n"
     ]
    }
   ],
   "source": [
    "# Python linear regression cost function and derivative - maybe just set random data points\n",
    "\n",
    "d = 2\n",
    "n = 3\n",
    "w = np.array([1, 2 ]).reshape(d, 1) # d x 1 vector\n",
    "X = np.array([[1,2], [1,3], [1,4]]) # n x d matrix\n",
    "y = np.array([2, 3, 5]).reshape(n, 1) # n x 1 vector\n",
    "print('Lets check the shapes: ', w.shape, X.shape, y.shape)\n",
    "print('Lets do the forward pass and compute Ein piece by piece')\n",
    "z1 = X@w  #Xw - outputs a vector ;; forward\n",
    "z2 = z1-y #outputs a vector\n",
    "z3 = z2.T @ z2 # outputs a scalar, Ein\n",
    "\n",
    "print('In sample error of w is z3 and is:', z3)\n",
    "print('The data X, y is fixed so z3 is a function of w only so to minimize we compute derivatives using the chain rule and return')\n",
    "print('The input is a function from R^d -> R - so the Jacobian should have shape 1 x d and the gradient d x 1')\n",
    "print('Lets use reverse the computation and compute derivatices by backpropagating and apply the chain rule')\n",
    "part_z3 = 2*z2.T # as defined the derivate of h(z) is 2z^t and we input z2, thus 2z2 ;; back\n",
    "part_z2 = np.eye(n) # The derivative of g was the the identity matrix of size n x n, the input is irrelevant\n",
    "part_z1 = X # the derivative of f was X regardless of input\n",
    "print('See if the shapes fit:', part_z1.shape, part_z2.shape, part_z3.shape)\n",
    "jacob_E = part_z3 @ part_z2 @ part_z1\n",
    "\n",
    "print('Shape of the Jacobiant:', jacob_E.shape)\n",
    "print('Shape of the Gradient:', jacob_E.T.shape)\n",
    "print('What is the Gradient: \\n', jacob_E.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Cost Functions\n",
    "\n",
    "You work at a company, and you have just released a new system based on\n",
    "machine learning using linear regression with the least squares error\n",
    "measure. You did not have time to look much at the final product but in\n",
    "testing everything looked fine. One of your colleagues, Clumsy, was\n",
    "responsible for running the learning algorithm on the final data you\n",
    "collected.\n",
    "\n",
    "The following morning Clumsy comes to you and says: <br><br>\n",
    "\n",
    "<div style=\"margin-left: 16px; \">\n",
    "   *I made an error on the few datapoints we received yesterday. I forgot to scale them down as\n",
    "we did with all the other data. But it is less than 1 percent of the\n",
    "data, so it is probably not a problem – the other data will dominate,\n",
    "right, since the error measure is an average.* </div>\n",
    "\n",
    "<b>Is clumsy right?</b>\n",
    "\n",
    "If you need help or would like to see the experiment in action read and run the script in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Weights: [3.4924179406186417, 7.0167511154115711]\n",
      "Model 2 Weights: [4.1893430154224296, 11.349585922506245]\n",
      "Model 1 Score (Error):  0.0864038470788\n",
      "Model 2 Score (Error):  10.500520741\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAHVCAYAAADVQH6wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niBhDmqmIiIiIMfwFzCvBHZBTKM0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ccad668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add python clumsy code\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "w = np.array([3, 7])\n",
    "n = 300\n",
    "z = np.random.rand(n).reshape(-1, 1)\n",
    "z.sort()\n",
    "X = np.c_[np.ones(n), z]\n",
    "y = X @ w + np.random.rand(n)\n",
    "new_n = int(n/100)\n",
    "new_z = np.random.rand(new_n).reshape(-1, 1)\n",
    "new_z.sort()\n",
    "new_X = np.c_[np.ones(new_n), new_z]\n",
    "new_y = 42*(new_X @ w + np.random.rand(new_n)) # scaled by 42\n",
    "from sklearn.linear_model import LinearRegression as lr # implement this yourself in coding exercise\n",
    "model1 = lr().fit(z, y)\n",
    "all_X = np.vstack((z, new_z))\n",
    "all_y = np.vstack((y.reshape(-1,1), new_y.reshape(-1, 1)))\n",
    "model2 = lr().fit(all_X, all_y.reshape(-1,))\n",
    "print('Model 1 Weights:', [model1.intercept_, model1.coef_[0]])\n",
    "print('Model 2 Weights:', [model2.intercept_, model2.coef_[0]])\n",
    "p1 = model1.predict(z)\n",
    "p2 = model2.predict(z)\n",
    "print('Model 1 Score (Error): ', np.mean((y-p1)**2))\n",
    "print('Model 2 Score (Error): ', np.mean((y-p2)**2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(z, y, c='b', label='Good Data')\n",
    "# plt.scatter(new_z, new_y, c='g', label='New Unscaled Data')\n",
    "plt.plot(z, p1, 'r-', label='Model 1')\n",
    "plt.plot(z, p2, 'g-', label='Model 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
