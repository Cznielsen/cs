{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Theoretical Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break Points and Growth Functions \n",
    "\n",
    "-   Is there always a break point for a finite hypothesis set of $n$\n",
    "    hypotheses? If so, can you give an upper bound? What is the growth\n",
    "    function?\n",
    "\n",
    "-   Does the set of all functions have a break point? What is its growth\n",
    "    function?\n",
    "\n",
    "-   What is the (smallest) break point for the hypothesis set consisting\n",
    "    of circles centered around $(0,0)$? For a given circle the\n",
    "    hypothesis returns $1$ for points inside the circle and $-1$ for\n",
    "    points outside. What is the growth function?\n",
    "\n",
    "-   What if we move to balls in the 3-dimensional space\n",
    "    ${{\\mathbb R}}^3$? Or in general $d$-dimensional space\n",
    "    ${{\\mathbb R}}^d$ (hyperspheres)?\n",
    "\n",
    "-  Show that the growth function for a singleton hypothesis set $H = \\{h\\}$ is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC Dimension \n",
    "\n",
    "-   Does VC Dimension depend on the learning algorithm or the actual\n",
    "    data set given?\n",
    "\n",
    "-   Does VC Dimension depend on the probability distribution generating\n",
    "    the data (not the labels)?\n",
    "\n",
    "-   If $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$ is\n",
    "    $VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)?$\n",
    "\n",
    "-   Can you give an upper bound on the VC dimension of a finite set of\n",
    "    $M$ hypotheses?\n",
    "\n",
    "-   What is the VC Dimension for the hypothesis set consisting of\n",
    "    circles centered around 0?\n",
    "\n",
    "-   What if we move to balls (3d)? or in general d dimensions\n",
    "    (hypershperes)?\n",
    "\n",
    "-   What is the maximal VC dimension possible of the intersection of\n",
    "    hypothesis spaces $\\mathcal{H}_1,\\dots,\\mathcal{H}_n$ with VC\n",
    "    dimensions $v_1,\\dots,v_n$?\n",
    "\n",
    "-   As previous question, instead what is the minimal VC dimension of\n",
    "    the union of the hypothesis spaces from the previous question?\n",
    "\n",
    "-   Book Problem 2.18 In short\n",
    "    $${{\\mathcal H}}= \\{h_\\alpha \\mid h_\\alpha(x) = (-1)^{\\lfloor \\alpha\n",
    "          x\\rfloor}, \\alpha \\in {{\\mathbb R}}\\}$$ Show that the VC\n",
    "    dimension of ${{\\mathcal H}}$ is infinite.\n",
    "\n",
    "    Hint: Use the points set\n",
    "    $x_1=10,x_2=100,\\dots,x_i = 10^i,\\dots,x_N=10^N$ and show how to\n",
    "    implement any dichotomy $y_1,\\dots,y_N$ (find $\\alpha$ that works).\n",
    "    You can safely assume $\\alpha >0$.\n",
    "    \n",
    "-   Show that the VC dimension of the hypothesis set consisting of axis aligned rectangles in $\\mathbb{R}^2$ is 4,\n",
    "    i.e. find a point set of 4 points you can shatter and argue that any point set of size 5 cannot be shattered.\n",
    "\n",
    "-   In Week 1 we showed that we could learn a Rectangle by computing the minimum bounding box with the following generalization bound\n",
    "    $$ P(E_\\textrm{out} > \\varepsilon) \\leq 4 e^{-\\varepsilon n/4} $$\n",
    "    Rewrite this bound and the VC dimension bound we get from the previous exercise to generalization bounds and compare\n",
    "    (the generalization bound -LFD 2.2.2- says that\n",
    "    $$\n",
    "    E_\\textrm{out} \\leq E_{in} + \\textrm{ something}\n",
    "    $$\n",
    "    with probability $1-\\delta$).\n",
    "    \n",
    "    Compare the two bounds. \n",
    "    Which is better and by how much? What differences are there?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VC Dimension of Hyperplanes (Book Exercise 2.4 p. 52)\n",
    "Show that the VC dimension of the hypothesis space $\\mathcal{H} = \\{\\textrm{sign}(w^\\intercal x) \\mid w\\in \\mathbb{R}^{d+1} \\}$\n",
    "\n",
    "We need to show \n",
    "1. That there exists a data set of size d+1 that can be shattered by hyperplanes\n",
    "2. That no data set of size d+2 can be shattered by hyperplanes\n",
    "\n",
    "We will give a few more hints than the book do.\n",
    "### Shattering d+1 points\n",
    "As the book hints you must create an \"easy\" data set that you store in matrix X. Here easy means X should be invertible.\n",
    "We suggest you consider the identity matrix with the first column set to ones (which it needs to be). This means we use one dimension for each point.\n",
    "Now you must show how to get any dichotomy, call that $y \\in \\{-1,1\\}^n$.\n",
    "To be precise you must find $w$ such that \n",
    "$$\n",
    "    y_i = \\textrm{sign}(w^\\intercal x_i) \n",
    "$$\n",
    "for $i=1,\\dots,d+1$.\n",
    "In matrix notation this is \n",
    "$$\n",
    "Y = \\textrm{Sign}(Xw)\n",
    "$$\n",
    "\n",
    "The final hint you get here. Make this nonlinear system of equations into a linear one and show how to get a $w$ that works.\n",
    "### No Shattering of d+2 points.\n",
    "Must show that for any d+2 points we must prove there is a  dichotomy hyperplanes can not capture.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Consider  d+2, $d+1$ dimensional points $x_1,\\dots, x_{d+2}$ and think of them as vectors.\n",
    "Since we have more vectors than dimensions the vectors must be linearly dependent.\n",
    "\n",
    "i.e. \n",
    "$$\n",
    "x_j = \\sum_{i\\neq j} a_i x_i\n",
    "$$\n",
    "Since $x_j$ is determined by the other points then so $w^\\intercal x_j$ for any $w$ determined by the points. This means the classification on point $x_j$ is dictated by the other points and thus cannot freely be chosen.\n",
    "I.e.\n",
    "$$\n",
    "w^\\intercal x_j = w^{\\intercal} \\sum{i\\neq j} a_i x_i =\\sum{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Define an impossible dichotomy as follows. \n",
    "$$\n",
    "y_i = \\textrm{sign}(a_i), \\quad i\\neq j, \\quad y_j = -1\n",
    "$$\n",
    "Show this dichotomy is impossible!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance \n",
    "\n",
    "-   Does Bias and Variance terms (two numbers) in the Bias Variance\n",
    "    decomposition depend on the learning algorithm?\n",
    "\n",
    "-   What is Variance (in Bias Variance trade-off) if we have a hypothesis\n",
    "    set of size $1$, namely the constant model $h(x) = 2$? (The learning\n",
    "    algorithm always picks this hypothesis no matter the data)\n",
    "\n",
    "-   What is the Variance (in the Bias Variance trade-off) if the simple\n",
    "    hypothesis from the previous question is replaced by a very very\n",
    "    sophisticated hypothesis?\n",
    "\n",
    "-   Assume that the target function is a degree 2 polynomial, and the\n",
    "    input to your algorithm is always a set of 11 (noiseless) points. Your\n",
    "    hypothesis set is the set of all degreee 10 polynomials and the\n",
    "    learning algorithm returns the hypothesis with the best fit\n",
    "    (minimizing least squared error) given the data. What is the Bias and what\n",
    "    is the Variance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Exercise 4.3 (p. 125)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bayes Error - The Optimal Classifier\n",
    "We consider 0-1 Loss i.e. $1_{f(x)\\neq h(x)}$\n",
    "\n",
    "Given a distribution $D$ over $X \\times Y$, the Bayes error $E^âˆ—$ is defined as the infimum of the errors achieved by measurable (technical term which you can interpret as \"reasonable\") functions $h: X \\rightarrow Y$:\n",
    "$$\n",
    "E^* = \\inf_h E_\\textrm{out}(h).\n",
    "$$\n",
    "A hypothesis $h$ with $E_{\\textrm{out}}(h) = E^*$ is called a Bayes hypothesis or Bayes classifier, which is the optimal classifier which achieves the optimal error.\n",
    "\n",
    "If the target function $f$ is deterministic (i.e. given input x it always outputs some fixed value y=f(x)) then what is the Bayes Error?\n",
    "\n",
    "Now let us consider a probabilistic target function. Assume the target probability distribution  is the following \"noisy function\" defined from a deterministic function $f: X\\rightarrow \\{0,1\\}$.\n",
    "\n",
    "$P(y \\mid x)$ is defined as follows.\n",
    "Given $x$ the compute value $f(x)$. With probability $\\theta=0.9$ return $f(x)$ and with probability $1-\\theta=0.1$ return $1-f(x)$\n",
    "\n",
    "What is the Bayes Error of this function? What is a Bayes classifier?\n",
    "What if we set $\\theta=0.5$ or $\\theta = 0.9$?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
