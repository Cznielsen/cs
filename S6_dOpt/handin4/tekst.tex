% Created 2018-05-07 Mon 15:33
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{bm}
\author{Christian Zhuang-Qing Nielsen\thanks{201504624, christian@czn.dk}}
\date{\today}
\title{dOpt Compulsory Assignment 4}
\hypersetup{
 pdfauthor={Christian Zhuang-Qing Nielsen},
 pdftitle={dOpt Compulsory Assignment 4},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.1 (Org mode 9.1.2)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{First Part}
\label{sec:orgb7e61af}
For this part we must show that there exists an optimal solution in the set of the corner points of the unit cube.

The trick here is that we can show that the objective function is linear in all cases. The only place where it might be quadratic is the second term. Since \(Q\) is defined to be \(0\) on its diagonal, we can write the objective function as two sums and call it \(f(x)\):
\begin{equation*}
\(f(x) = \sum_i c_ix_i + \frac{1}{2}\sum_{i,j}q_{ij}x_ix_j\)
\end{equation}
where each term in the sum where \(i=j\) is \(0\). This way, it cannot possibly become quadratic. \newline \newline
\textbf{Some notation:} \([x,y]\) means an interval between \(x\) and \(y\). \(\{x,y\}\) means a set consisting of \(x\) and \(y\). \newline \newline
If we assume we are given an optimal solution \(x \in [0,1]^n\) where \(k\) of its coordinates that aren't \(0\) or \(1\) (and thus not on the corners of the unit cube) then there must exist another solution \(x' \in [0,1]^n\) where \(k-1\) of its coordinates aren't \(0\) or \(1\). Given a coordinate \(k_i\) from the summations earlier we let \(k_i \neq {0,1}\). We can then differentiate the function \(f\) with respect to this:
\begin{equation*}
\frac{\delta f}{\delta k_i} = c\(_{\text{i}}\)
\end{equation}
Since we have derived a linear function the derivative can be one of the three following cases: \newline \newline
\textbf{Cases:} \newline
\(c_i = 0 \implies k_i = 0\) \newline 
\(c_i > 0 \implies k_i = 0\) \newline
\(c_i < 0 \implies k_i = 1\) \newline \newline
For the first case we have a constant function where any point on the linear function in the unit cube is an optimal solution. However, in case two and three the function is either increasing or decreasing, which means that we can reach an even better solution by setting the \(k_i\) to one of the corner points. If it is increasing we will set it to \(0\) and if it is decreasing we set it to \(1\). \newline \newline
This is a contradition towards our assumption that an optimal solution was not on the corner points. We can the iterate this method to do it on all assumed optimal solutions until there are no coordinates which aren't \(0\) or \(1\), which leaves us with an optimal solution to the quadratic program in the set \(\{0,1\}^n\) of corner points of the unit cube.

\section{Second Part}
\label{sec:org068afee}
\begin{equation*}
\(f(x)=-\sum_{i \in V}x_i + \sum_{(i,j) \in  E} n \cdot x_i x_j\)
\end{equation}
For this proof we must show that \(f(x) \geq 0\) when \(S\) is not an independent set and that \(f(x) = -|S|\) when \(S\) \emph{is} an independent set.

Our equation \(f(x)\) consists of two summation terms.
When \(S\) is not independent we have to cases to work with: The first case is that the sets are empty and the resulting output is 0. The second case is when the rightmost sum of \(f(x)\) must be at least \(n\) as there must exists at least one connection between two vertices. Since \(n\) is defined to be the size of \(V\) and \(S \subset V\), it must necessarily be bigger than the first term as \(n+(-1) \geq 0\) always. \newline \newline
If \(S\) is an independent set, there must be no edges between the vertices in the graph, which results in every term in the sum of the second term of \(f(x)\) resulting in 0 (as there are no connections in an independent set). This leaves us with the first term, which is a sum of ones and zeros, where it is only incremented when a vertex is in the subset \(S\). This results in the output being the negative size of \(S\), which is exactly \(-|S|\).
\section{Third Part}
\label{sec:org5dc1b15}
Since we already have the independent set problem we will utilise this by converting it to the MAX INDEPENDENT SET-problem. This is done by trying to minimise the output of the function \(f(x)\), which is the same as increasing the size of the independent set. As see in the first part, we can rewrite the quadratic function two the sum of two terms (which are summations in themselves), which means we can basically utilise our independent set function \(f(x)\) as the objective function in our quadratic program:
\begin{equation*}
\(min \quad f(x)\)
\end{equation}
Since we cannot assume that our quadratic function is positive definite (and thus convex), we can utilise our knowledge from part one to reach a general optimal solution to our quadratic program. This means we have reduced to a quadratic programming problem from a independent set problem:
\newline \newline
INDEPENDENT SET \(\leq\) QUADRATIC PROGRAMMING
\newline \newline
Since INDEPENDENT SET is NP-complete and we reduce it to a QUADRATIC PROGRAMMING-problem, which is not a decidability problem and is at least as hard as the hardest NP-complete problem it is a NP-hard problem.
\end{document}
